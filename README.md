# Fine-Tuning UniChart on ChartQA dataset

UniChart fine-tuning on ChartQA dataset.
Вступление
В данном ноутбуке представлен процесс дообучения модели UniChart на датасете ChartQA.

Цель: решение задачи Chart Question Answering с помощью VLM (Vision Language Model)

Ссылки на ресурсы и авторов модели и датасета, использованных в ходе работы будут в конце ноутбука.

Структура VLM
Архитектура Vision Language Models (VLM)
1. Кодировщик изображений (Image Encoder):

Кодирует изображение в векторное представление, которое может быть обработано нейронной сетью. Обычно используется модель типа CNN или трансформер.
2. Кодировщик текста (Text Encoder):

Обрабатывает текстовые данные и преобразует их в векторные представления. Обычно это трансформерная модель, такая как BERT или GPT.
3. Мультимодальный интегратор (Multimodal Integrator):

Объединяет представления изображений и текста в единое векторное пространство. Это может быть реализовано с помощью различных методов, включая конкатенацию, внимание (attention) и другие нейронные сети.
4. Декодер (Decoder):

Генерирует текст на основе мультимодального представления. Часто используется трансформерная архитектура для декодирования.
Обучение Vision Language Models
1. Предобучение:

Модели проходят предобучение на больших наборах данных изображений и текста, чтобы научиться основным представлениям. Это включает в себя использование пар изображение-текст, таких как подписи к изображениям.
2. Совместное обучение (Fine-Tuning):

Модели дообучаются на конкретных задачах, таких как описание изображений, визуальный вопрос-ответ (VQA) или генерация текста по изображению. Это помогает модели адаптироваться к конкретным задачам и улучшать свою производительность.
Пример моделей:
LLaVA:

Использует CLIP для кодирования изображений и Vicuna как текстовый декодер. Проходит через этапы генерации вопросов и ответов с помощью GPT-4, после чего дообучается.
KOSMOS-2:

Модель обучается "end-to-end", объединяя все компоненты в единый процесс обучения.
Основные этапы обучения:
Сбор данных: Пары изображение-текст.
Предобучение: Обучение моделей на больших объемах данных.
Файнтюнинг: Точная настройка на специфические задачи.
Описание модели
UniChart - это VLM (Vision Language Model), обученная специально для работы с графиками. Изначально она способна эффективно суммировать информацию по графикам, однако после дообучения на датасете ChartQA дает уверенные и точные ответы на однозначные вопросы по графикам.

Использует в своей архитектуре следующие элементы: Model: VisionEncoderDecoderModel Processor: DonutProcessor Config: BartConfig

Отсюда ясно видно принцип работы VLM.

VLM - это модели, которые могут одновременно изучать изображения и текст для решения многих задач, от визуальных ответов на вопросы до создания субтитров к изображениям.

VLM в широком смысле определяются как мультимодальные модели, которые могут обучаться на основе изображений и текста. Это тип генеративных моделей, которые принимают изображения и текст на входе и генерируют текст на выходе. Варианты использования включают обсуждение изображений в чате, распознавание изображений с помощью инструкций, визуальные ответы на вопросы, понимание документов, создание субтитров к изображениям и другие. Эти модели могут выводить ограничивающие рамки или маски сегментации, когда им предлагается обнаружить или сегментировать конкретный объект, или они могут локализовать различные объекты или отвечать на вопросы об их относительном или абсолютном расположении.
